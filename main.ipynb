{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "mount_file_id": "1vR3rUWAymV6-PGSES9rb0TPDhlDS_N4v",
      "authorship_tag": "ABX9TyOvxtyTkvKtTUpg7JabDLrc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DunkleCat/a3c-tensorflow/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj5ZKxV0N9Th"
      },
      "source": [
        "# Header\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaSHRNgfD45r"
      },
      "source": [
        "## Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOyaCBsXOFh2"
      },
      "source": [
        "!pip install tf-agents\n",
        "\n",
        "import csv\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers as layers\n",
        "from tf_agents.utils import value_ops\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from threading import Thread, Lock\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "# device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "# print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wL_jE09y5--"
      },
      "source": [
        "## Global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqbWBln7y9Xi"
      },
      "source": [
        "def getNumberOfWorkers():\n",
        "  return cpu_count()\n",
        "  # return 1\n",
        "  \n",
        "def getMaxEpisodes():\n",
        "  return 10000\n",
        "\n",
        "def getBatchSize():\n",
        "  return 5\n",
        "\n",
        "def env_name():\n",
        "  return \"CartPole-v1\"\n",
        "\n",
        "def getTypology():\n",
        "  return (\"classic\",)\n",
        "  # Options:\n",
        "  # (\"classic\",)\n",
        "  # (\"atari\", \"visual\")\n",
        "  # (\"atari\", \"ram\")\n",
        "\n",
        "CUR_EPISODE = 0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2XgI0qL0o48"
      },
      "source": [
        "# Random Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hwhP4Uu0q2S"
      },
      "source": [
        "import gym\n",
        "env = gym.make(env_name())\n",
        "for i_episode in range(20):\n",
        "  observation = env.reset()\n",
        "  episode_reward, done = 0, False\n",
        "  while not done:\n",
        "    # env.render()\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    episode_reward += reward\n",
        "  print(\"Episode finished with a reward of {}\".format(episode_reward))\n",
        "  episode_reward = 0\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdw-FSR4D7OH"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDgVaM2kEBTg"
      },
      "source": [
        "def main():\n",
        "  agent = Agent()\n",
        "  agent.train()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X18YTkTZOEa2"
      },
      "source": [
        "# ActorCritic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAiJJ5n971dC"
      },
      "source": [
        "## Help functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYAvudDf70wN"
      },
      "source": [
        "def get_input_classic(input, single):\n",
        "  \n",
        "  if not single:\n",
        "    return input\n",
        "  else:\n",
        "    shape = input.shape\n",
        "    n_samples = 1\n",
        "    new_input = np.ndarray((n_samples,\n",
        "                            shape[0]))\n",
        "    new_input[0] = input\n",
        "    return new_input\n",
        "\n",
        "def get_input_atari(input, single):\n",
        "\n",
        "  if single:\n",
        "    shape = input.shape\n",
        "    n_samples = 1\n",
        "  else:\n",
        "    shape = input[0].shape\n",
        "    n_samples = len(input)\n",
        "\n",
        "  if getTypology()[1] is \"visual\":\n",
        "    new_input = np.ndarray((n_samples,\n",
        "                            1,\n",
        "                            shape[0],\n",
        "                            shape[1],\n",
        "                            shape[2]))\n",
        "  elif getTypology()[1] is \"ram\":\n",
        "    new_input = np.ndarray((n_samples,\n",
        "                            1,\n",
        "                            shape[0]))\n",
        "\n",
        "  if single:\n",
        "      new_input[0][0] = input\n",
        "  else:\n",
        "    for k in range(len(input)):\n",
        "        new_input[k][0] = input[k]\n",
        "\n",
        "  return new_input\n",
        "\n",
        "def init_state_shape(state_shape):\n",
        "  if getTypology()[0] is \"classic\":\n",
        "    return state_shape\n",
        "  elif getTypology()[1] is \"visual\":\n",
        "      return np.ndarray(shape = (1,\n",
        "                                 state_shape[0],\n",
        "                                 state_shape[1],\n",
        "                                 state_shape[2])).shape\n",
        "  elif getTypology()[1] is \"ram\":\n",
        "    return np.ndarray(shape = (1,\n",
        "                               state_shape[0])).shape"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JiTpFEarvEO"
      },
      "source": [
        "## ActorCritic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxcQLC8kYZ-9"
      },
      "source": [
        "class ActorCritic:\n",
        "\n",
        "  def __init__(self, state_shape, action_shape):\n",
        "    self.state_shape = init_state_shape(state_shape)    \n",
        "    self.action_shape = action_shape\n",
        "    self.model = create_model(state_shape, action_shape)\n",
        "    self.opt = tf.keras.optimizers.Adam(0.0005)\n",
        "\n",
        "  def get_action(self, input):\n",
        "    input = eval(\"get_input_\" + getTypology()[0] + \"(input, single = True)\")\n",
        "    action_dist, _ = self.model.predict(input)\n",
        "    return np.random.choice(self.action_shape, p = action_dist[0])\n",
        "\n",
        "  def get_value(self, input, single):\n",
        "    input = eval(\"get_input_\" + getTypology()[0] + \"(input, single = single)\")\n",
        "    _, v = self.model.predict(input)\n",
        "    return v\n",
        "\n",
        "  def train(self, input, actions, advantages, discounted_rewards):\n",
        "\n",
        "    def compute_loss(actions, action_dist, advantages, v_pred, discounted_rewards):\n",
        "      # Compute policy loss\n",
        "      scc = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "      policy_loss = 0.5 * scc(actions, action_dist, sample_weight = tf.stop_gradient(advantages))\n",
        "      # Compute entropy\n",
        "      cc = tf.keras.losses.CategoricalCrossentropy()\n",
        "      entropy = 0.01 * cc(action_dist, action_dist)\n",
        "      # Compute value loss\n",
        "      mse = tf.keras.losses.MeanSquaredError()\n",
        "      value_loss = mse(v_pred, discounted_rewards)\n",
        "\n",
        "      return policy_loss + value_loss - entropy\n",
        "  \n",
        "    with tf.GradientTape() as tape:\n",
        "      input = eval(\"get_input_\" + getTypology()[0] + \"(input, single = False)\")\n",
        "      action_dist, v_pred = self.model(input, training = True)\n",
        "      loss = compute_loss(actions, action_dist, advantages, v_pred, discounted_rewards)\n",
        "    grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "    self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT1ZYamwoibJ"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVmYqHAVlVjC"
      },
      "source": [
        "def create_model(input_shape, output_shape):\n",
        "  t = getTypology()[0]\n",
        "  if t is \"classic\":\n",
        "    return ClassicModel(input_shape, output_shape)\n",
        "  elif t is \"ram\":\n",
        "    return RamModel(input_shape, output_shape)\n",
        "  elif t is \"visual\":\n",
        "    return VisualModel(input_shape, output_shape)\n",
        "  else:  \n",
        "    raise ValueError(\"Bad model typology\")\n",
        "\n",
        "  # function_name = \"create_model_\" + getTypology()[0]\n",
        "  # function_arguments = \"(input_shape, output_shape, \\\"softmax\\\", \\\"None\\\")\"\n",
        "  # return eval(function_name + function_arguments)\n",
        "\n",
        "def create_model_atari(input_shape, output_shape, activation_function):\n",
        "  if getTypology()[1] is \"visual\":\n",
        "    return keras.Sequential(\n",
        "        [\n",
        "         layers.Dropout(0.5),\n",
        "         layers.Dense(256,\n",
        "                      activation=\"relu\"),\n",
        "         # layers.Dense(128,\n",
        "         #              activation=\"relu\"),\n",
        "         # layers.Dense(64,\n",
        "         #              activation=\"relu\"),\n",
        "         layers.Dense(output_shape, \n",
        "                      activation = activation_function)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "class Vision(layers.Layer):\n",
        "\n",
        "  def __init__(self, name = \"visual\", **kwargs):\n",
        "    super(Vision, self).__init__(name = name, **kwargs)\n",
        "    self.rnn1 = layers.ConvLSTM2D(16,4,2, \n",
        "                                  data_format='channels_last',\n",
        "                                  dropout = 0.5,\n",
        "                                  return_sequences = True)\n",
        "    self.rnn2 = layers.ConvLSTM2D(32,4,1, \n",
        "                                  data_format='channels_last',\n",
        "                                  dropout = 0.5,\n",
        "                                  return_sequences = True)\n",
        "    self.norm = layers.BatchNormalization()\n",
        "    self.avrg = layers.AveragePooling2D()\n",
        "    self.flat = layers.Flatten()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.rnn1(inputs)\n",
        "    x = self.rnn2(x)\n",
        "    x = self.norm(x)\n",
        "    x = self.avrg(x)\n",
        "    return self.flat(x)\n",
        "\n",
        "class VisualModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, input_shape, output_shape):\n",
        "    super(RamModel, self).__init__()\n",
        "    self.eye = Vision()\n",
        "    self.dens = layers.Dense(256, activation = \"relu\")\n",
        "    self.probs = layers.Dense(output_shape, activation = \"softmax\")\n",
        "    self.value = layers.Dense(1, activation = \"linear\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.eye(inputs)\n",
        "    x = self.dens(x)\n",
        "    p = self.probs(x)\n",
        "    v = self.value(x)\n",
        "    return p, v\n",
        "\n",
        "class RamModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, input_shape, output_shape):\n",
        "    super(RamModel, self).__init__()\n",
        "    self.lstm1 = layers.LSTM(32)\n",
        "    self.dens1 = layers.Dense(32, activation = \"relu\")\n",
        "    self.dens2 = layers.Dense(64, activation = \"relu\")\n",
        "    self.dens3 = layers.Dense(32, activation = \"relu\")\n",
        "    self.probs = layers.Dense(output_shape, activation = \"softmax\")\n",
        "    self.value = layers.Dense(1, activation = \"linear\")\n",
        "\n",
        "  def call(self, inputs, training = False):\n",
        "    x = self.lstm1(inputs)\n",
        "    x = self.dens1(x)\n",
        "    x = self.dens2(x)\n",
        "    x = self.dens3(x)\n",
        "    p = self.probs(x)\n",
        "    v = self.value(x)\n",
        "    return p, v\n",
        "    \n",
        "class ClassicModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, input_shape, output_shape):\n",
        "    super(ClassicModel, self).__init__()\n",
        "    self.dens1 = layers.Dense(32, activation = \"relu\")\n",
        "    self.norm1 = layers.BatchNormalization()\n",
        "    self.drop1 = layers.Dropout(0.2)\n",
        "    self.dens2 = layers.Dense(32, activation = \"relu\")\n",
        "    self.dens3 = layers.Dense(16, activation = \"relu\")\n",
        "    self.probs = layers.Dense(output_shape, activation = \"softmax\")\n",
        "    self.value = layers.Dense(1, activation = \"linear\")\n",
        "\n",
        "  def call(self, inputs, training = False):\n",
        "    x = self.dens1(inputs)\n",
        "    x = self.norm1(x)\n",
        "    if training:\n",
        "      x = self.drop1(x)\n",
        "    # x = self.dens2(x)\n",
        "    # x = self.dens3(x)\n",
        "    p = self.probs(x)\n",
        "    v = self.value(x)\n",
        "    return p, v"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEiSKn2WORcL"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bghH0YluOXxI"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        env = gym.make(env_name())\n",
        "        self.state_shape = env.observation_space.shape\n",
        "        self.action_shape = env.action_space.n\n",
        "        env.close()\n",
        "        \n",
        "        self.global_actor_critic = ActorCritic(self.state_shape, self.action_shape)\n",
        "\n",
        "    def train(self):\n",
        "        workers = []\n",
        "\n",
        "        with open(\"/content/drive/My Drive/Machine Learning/logs/\" + str(env_name()) + \"_loss\", \"w\") as csv_file:\n",
        "          fieldnames = ['episode', 'loss']\n",
        "          csv_writer = csv.DictWriter(csv_file, fieldnames = fieldnames)\n",
        "          csv_writer.writeheader()\n",
        "        with open(\"/content/drive/My Drive/Machine Learning/logs/\" + str(env_name()) + \"_reward\", \"w\") as csv_file:\n",
        "          fieldnames = ['episode', 'reward']\n",
        "          csv_writer = csv.DictWriter(csv_file, fieldnames = fieldnames)\n",
        "          csv_writer.writeheader()\n",
        "\n",
        "        for i in range(getNumberOfWorkers()):\n",
        "            workers.append(WorkerAgent(self.global_actor_critic))\n",
        "        for worker in workers:\n",
        "            worker.start()\n",
        "\n",
        "        for worker in workers:\n",
        "            worker.join()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iacLy4soOTSE"
      },
      "source": [
        "# WorkerAgent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJONQe_4Oc7_"
      },
      "source": [
        "class WorkerAgent(Thread):\n",
        "    def __init__(self, global_actor_critic):\n",
        "        Thread.__init__(self)\n",
        "        self.lock = Lock()\n",
        "        self.env = gym.make(env_name())\n",
        "        self.state_shape = env.observation_space.shape\n",
        "        self.action_shape = env.action_space.n\n",
        "\n",
        "        self.global_actor_critic = global_actor_critic\n",
        "        self.actor_critic = ActorCritic(self.state_shape, self.action_shape)\n",
        "        \n",
        "        self.actor_critic.model.set_weights(self.global_actor_critic.model.get_weights())\n",
        "        \n",
        "    def train(self):\n",
        "\n",
        "        def list_to_batch(list):\n",
        "            batch = list[0]\n",
        "\n",
        "            for elem in list[1:]:\n",
        "                batch = np.append(batch, elem, axis = 0)\n",
        "            return batch\n",
        "\n",
        "        global CUR_EPISODE\n",
        "\n",
        "        while getMaxEpisodes() >= CUR_EPISODE:\n",
        "            state_batch = []\n",
        "            action_batch = []\n",
        "            reward_batch = []\n",
        "            episode_reward, episode_loss, done = 0, 0, False\n",
        "\n",
        "            state = self.env.reset()\n",
        "\n",
        "            while not done:\n",
        "                # self.env.render()\n",
        "                action = self.actor_critic.get_action(state)\n",
        "                next_state, reward, done, _ = self.env.step(action) \n",
        "                episode_reward += reward\n",
        "\n",
        "                reward = np.reshape(reward, [1, 1])\n",
        "\n",
        "                state_batch.append(state)\n",
        "                action_batch.append(action)\n",
        "                reward_batch.append(reward)\n",
        "\n",
        "                if len(state_batch) >= getBatchSize() or done:\n",
        "                    actions = np.array(action_batch)\n",
        "                    states = np.array(state_batch)\n",
        "                    rewards = np.array(reward_batch)\n",
        "                    \n",
        "                    final_value = self.actor_critic.get_value(next_state, single = True)\n",
        "                    discounted_rewards = np.ones(rewards.shape)\n",
        "                    discounted_rewards[-1] = 0 if done else 0.99 * final_value\n",
        "                    for i in reversed(range(1, len(rewards)-1)):\n",
        "                      discounted_rewards[i] = 0.99 * discounted_rewards[i+1] + rewards[i]\n",
        "\n",
        "                    values = self.actor_critic.get_value(states, single = False)\n",
        "                    advantages = discounted_rewards - values\n",
        "                    # print(advantages)                    \n",
        "\n",
        "                    with self.lock:\n",
        "                        loss = self.global_actor_critic.train(states, actions, advantages, discounted_rewards)\n",
        "                        self.actor_critic.model.set_weights(self.global_actor_critic.model.get_weights())\n",
        "                        \n",
        "                        episode_loss += loss\n",
        "        \n",
        "                    state_batch = []\n",
        "                    action_batch = []\n",
        "                    reward_batch = []\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            if CUR_EPISODE%10 is 0:\n",
        "              print(\"EP{}: Reward = {}, Loss = {}\".format(CUR_EPISODE, \n",
        "                                                          episode_reward, \n",
        "                                                          episode_loss))\n",
        "              self.global_actor_critic.model.save(\"/content/drive/My Drive/Machine Learning/models/\" + str(env_name()))\n",
        "              \n",
        "              with open(\"/content/drive/My Drive/Machine Learning/logs/\" + str(env_name()) + \"_loss\", \"w\") as csv_file:\n",
        "                fieldnames = ['episode', 'loss']\n",
        "                csv_writer = csv.DictWriter(csv_file, fieldnames = fieldnames)\n",
        "                csv_writer.writerow({'episode': CUR_EPISODE, 'loss': episode_actor_loss})\n",
        "              with open(\"/content/drive/My Drive/Machine Learning/logs/\" + str(env_name()) + \"_reward\", \"w\") as csv_file:\n",
        "                fieldnames = ['episode', 'reward']\n",
        "                csv_writer = csv.DictWriter(csv_file, delimiter=',')\n",
        "                csv_writer.writerow({'episode': CUR_EPISODE, 'reward': episode_reward})\n",
        "              \n",
        "            CUR_EPISODE += 1\n",
        "\n",
        "    def run(self):\n",
        "        self.train()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_uiCGZBOea_"
      },
      "source": [
        "# Entrypoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLtYNO7PD_ZC"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "else:\n",
        "  main()\n",
        "\n",
        "# Load model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}