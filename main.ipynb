{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "mount_file_id": "1vR3rUWAymV6-PGSES9rb0TPDhlDS_N4v",
      "authorship_tag": "ABX9TyMEluFE/n71jIQlbln0fwu8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DunkleCat/a3c-tensorflow/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj5ZKxV0N9Th"
      },
      "source": [
        "# Header\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaSHRNgfD45r"
      },
      "source": [
        "## Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOyaCBsXOFh2",
        "outputId": "65aeab9d-8b4b-497f-dee5-a6cf50802589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!pip install tf-agents\n",
        "\n",
        "import csv\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers as layers\n",
        "from tf_agents.utils import value_ops\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from threading import Thread, Lock\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-agents\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/77/df0c0ca6f0b1a59b812d81d7737d8ea2a95d8716f9ffc1a68822531b78fb/tf_agents-0.6.0-py3-none-any.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 19.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 6.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 7.3MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 8.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 8.3MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 8.9MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122kB 8.3MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 8.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 8.3MB/s eta 0:00:01\r\u001b[K     |████▌                           | 153kB 8.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 163kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 184kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 194kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 204kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 215kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 225kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 235kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 245kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 256kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 266kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 286kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 296kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 307kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 317kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 327kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 337kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 348kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 358kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 368kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 389kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 399kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 409kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 419kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 430kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 440kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 450kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 460kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 471kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 491kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 501kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 512kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 522kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 532kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 542kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 552kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 563kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 573kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 583kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 593kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 604kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 614kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 624kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 634kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 645kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 655kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 665kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 675kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 686kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 696kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 706kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 716kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 727kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 737kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 747kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 757kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 768kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 778kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 788kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 798kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 808kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 819kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 829kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 839kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 849kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 860kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 870kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 880kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 890kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 901kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 911kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 921kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 931kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 942kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 952kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 962kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 972kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 983kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 993kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0MB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.0MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.0MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.0MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.18.5)\n",
            "Requirement already satisfied: gin-config>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-probability>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.11.0)\n",
            "Requirement already satisfied: cloudpickle==1.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-agents) (0.10.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.11.0->tf-agents) (0.1.5)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.11.0->tf-agents) (0.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.11.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.11.3->tf-agents) (50.3.0)\n",
            "Installing collected packages: tf-agents\n",
            "Successfully installed tf-agents-0.6.0\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wL_jE09y5--"
      },
      "source": [
        "## Global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqbWBln7y9Xi"
      },
      "source": [
        "def getNumberOfWorkers():\n",
        "  return cpu_count()\n",
        "  # return 1\n",
        "\n",
        "def getMaxEpisodes():\n",
        "  return 1000\n",
        "\n",
        "def getBatchSize():\n",
        "  return 10\n",
        "\n",
        "def env_name():\n",
        "  return \"CartPole-v1\"\n",
        "\n",
        "def getTypology():\n",
        "  return (\"classic\",)\n",
        "  # Options:\n",
        "  # (\"classic\",)\n",
        "  # (\"atari\", \"visual\")\n",
        "  # (\"atari\", \"ram\")\n",
        "\n",
        "CUR_EPISODE = 0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2XgI0qL0o48"
      },
      "source": [
        "# Random Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hwhP4Uu0q2S",
        "outputId": "345b95aa-3833-4145-808e-67af40014b8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "import gym\n",
        "env = gym.make(env_name())\n",
        "for i_episode in range(20):\n",
        "  observation = env.reset()\n",
        "  episode_reward, done = 0, False\n",
        "  while not done:\n",
        "    # env.render()\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    episode_reward += reward\n",
        "  print(\"Episode finished with a reward of {}\".format(episode_reward))\n",
        "  episode_reward = 0\n",
        "env.close()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode finished with a reward of 15.0\n",
            "Episode finished with a reward of 46.0\n",
            "Episode finished with a reward of 15.0\n",
            "Episode finished with a reward of 16.0\n",
            "Episode finished with a reward of 24.0\n",
            "Episode finished with a reward of 20.0\n",
            "Episode finished with a reward of 8.0\n",
            "Episode finished with a reward of 18.0\n",
            "Episode finished with a reward of 22.0\n",
            "Episode finished with a reward of 16.0\n",
            "Episode finished with a reward of 12.0\n",
            "Episode finished with a reward of 15.0\n",
            "Episode finished with a reward of 15.0\n",
            "Episode finished with a reward of 38.0\n",
            "Episode finished with a reward of 13.0\n",
            "Episode finished with a reward of 18.0\n",
            "Episode finished with a reward of 23.0\n",
            "Episode finished with a reward of 23.0\n",
            "Episode finished with a reward of 10.0\n",
            "Episode finished with a reward of 23.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdw-FSR4D7OH"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDgVaM2kEBTg"
      },
      "source": [
        "def main():\n",
        "  agent = Agent()\n",
        "  agent.train()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X18YTkTZOEa2"
      },
      "source": [
        "# ActorCritic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAiJJ5n971dC"
      },
      "source": [
        "## Help functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYAvudDf70wN"
      },
      "source": [
        "def get_input_classic(input, single):\n",
        "  \n",
        "  if not single:\n",
        "    return input\n",
        "  else:\n",
        "    shape = input.shape\n",
        "    n_samples = 1\n",
        "    new_input = np.ndarray((n_samples,\n",
        "                            shape[0]))\n",
        "    new_input[0] = input\n",
        "    return new_input\n",
        "\n",
        "def get_input_atari(input, single):\n",
        "\n",
        "  if single:\n",
        "    shape = input.shape\n",
        "    n_samples = 1\n",
        "  else:\n",
        "    shape = input[0].shape\n",
        "    n_samples = len(input)\n",
        "\n",
        "  if getTypology()[1] is \"visual\":\n",
        "    new_input = np.ndarray((n_samples,\n",
        "                            1,\n",
        "                            shape[0],\n",
        "                            shape[1],\n",
        "                            shape[2]))\n",
        "  elif getTypology()[1] is \"ram\":\n",
        "    new_input = np.ndarray((n_samples,\n",
        "                            1,\n",
        "                            shape[0]))\n",
        "\n",
        "  if single:\n",
        "      new_input[0][0] = input\n",
        "  else:\n",
        "    for k in range(len(input)):\n",
        "        new_input[k][0] = input[k]\n",
        "\n",
        "  return new_input\n",
        "\n",
        "def init_state_shape(state_shape):\n",
        "  if getTypology()[0] is \"classic\":\n",
        "    return state_shape\n",
        "  elif getTypology()[1] is \"visual\":\n",
        "      return np.ndarray(shape = (1,\n",
        "                                 state_shape[0],\n",
        "                                 state_shape[1],\n",
        "                                 state_shape[2])).shape\n",
        "  elif getTypology()[1] is \"ram\":\n",
        "    return np.ndarray(shape = (1,\n",
        "                               state_shape[0])).shape"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JiTpFEarvEO"
      },
      "source": [
        "## Actor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxcQLC8kYZ-9"
      },
      "source": [
        "class Actor:\n",
        "\n",
        "  def __init__(self, state_shape, action_shape):\n",
        "    self.state_shape = init_state_shape(state_shape)    \n",
        "    self.action_shape = action_shape\n",
        "    self.model = create_model(self.state_shape, self.action_shape)\n",
        "    self.opt = tf.keras.optimizers.Adam(0.0005)\n",
        "\n",
        "  def get_action(self, input):\n",
        "    input = eval(\"get_input_\" + getTypology()[0] + \"(input, single = True)\")\n",
        "    action_distribution = self.model.predict(input)\n",
        "    return np.random.choice(self.action_shape, p = action_distribution[0])\n",
        "    # return np.argmax(action_distribution == action)\n",
        "\n",
        "  def train(self, input, actions, advantages):\n",
        "\n",
        "    def compute_loss(actions, action_dist, advantages):\n",
        "      # Compute policy loss\n",
        "      scc = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "      # print(advantages)\n",
        "      policy_loss = scc(actions, action_dist, sample_weight = tf.stop_gradient(advantages))\n",
        "      # tmp = tf.one_hot(actions,len(action_dist[0]),dtype=tf.float64)\n",
        "      # tmp = tf.reduce_sum(tf.math.multiply(tmp, action_dist))\n",
        "      # tmp = tf.math.log(tmp)\n",
        "      # policy_loss = tf.reduce_sum(tmp * advantages)\n",
        "      # Compute entropy\n",
        "      cc = tf.keras.losses.CategoricalCrossentropy()\n",
        "      entropy = 0.01 * cc(action_dist, action_dist)\n",
        "      # entropy = 0.001 * tf.reduce_sum(action_dist * tf.math.log(action_dist))\n",
        "      return policy_loss - entropy\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      input = eval(\"get_input_\" + getTypology()[0] + \"(input, single = False)\")\n",
        "      # input = get_input(input)\n",
        "      action_dist = self.model(input, training = True)\n",
        "      loss = compute_loss(actions, action_dist, advantages)\n",
        "    grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "    self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "    return loss"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0rD3US-rtVC"
      },
      "source": [
        "## Critic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pK8YGjpk_fp"
      },
      "source": [
        "class Critic:\n",
        "  def __init__(self, state_shape):\n",
        "    self.state_shape = init_state_shape(state_shape)\n",
        "    self.model = create_model(self.state_shape, 1)\n",
        "    self.opt = tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "  def get_value(self, input, single):\n",
        "    input = eval(\"get_input_\" + getTypology()[0] + \"(input, single = single)\")\n",
        "    return self.model.predict(input)\n",
        "\n",
        "  def train(self, input, td_targets):\n",
        "\n",
        "    def compute_loss(v_pred, v_targets):\n",
        "      mse = tf.keras.losses.MeanSquaredError()\n",
        "      return mse(v_pred, v_targets)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      input = eval(\"get_input_\" + getTypology()[0] + \"(input, single = False)\")\n",
        "      v_pred = self.model(input, training=True)\n",
        "      assert v_pred.shape == td_targets.shape\n",
        "      loss = compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
        "    grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "    self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "    return loss"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT1ZYamwoibJ"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVmYqHAVlVjC"
      },
      "source": [
        "def create_model(input_shape, output_shape):\n",
        "  \n",
        "  if output_shape is 1: #Critic\n",
        "    activation_function = \"relu\"\n",
        "  else: # Actor\n",
        "    activation_function = \"softmax\"\n",
        "\n",
        "  function_name = \"create_model_\" + getTypology()[0]\n",
        "  function_arguments = \"(input_shape, output_shape, activation_function)\"\n",
        "  return eval(function_name + function_arguments)\n",
        "\n",
        "def create_model_atari(input_shape, output_shape, activation_function):\n",
        "  if getTypology()[1] is \"visual\":\n",
        "    return keras.Sequential(\n",
        "        [\n",
        "         layers.InputLayer(input_shape),\n",
        "         layers.ConvLSTM2D(16,4,2, \n",
        "                           data_format='channels_last',\n",
        "                           dropout = 0.5,\n",
        "                           #recurrent_dropout = 0.5,\n",
        "                           return_sequences = True),\n",
        "         layers.BatchNormalization(),\n",
        "         # layers.ConvLSTM2D(32,4,1, \n",
        "         #                   data_format='channels_last',\n",
        "         #                   dropout = 0.5,\n",
        "         #                   recurrent_dropout = 0.5,\n",
        "         #                   return_sequences = True),\n",
        "         # layers.BatchNormalization(),\n",
        "         layers.ConvLSTM2D(32,4,1, \n",
        "                           data_format='channels_last',\n",
        "                           dropout = 0.5,\n",
        "                           #recurrent_dropout = 0.5,\n",
        "                           return_sequences = True),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.ConvLSTM2D(1,1,1, \n",
        "                           data_format='channels_last',\n",
        "                           dropout = 0.5,\n",
        "                           recurrent_dropout = 0.5,\n",
        "                           return_sequences = False),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.AveragePooling2D(),\n",
        "         layers.Flatten(),\n",
        "         layers.Dropout(0.5),\n",
        "         layers.Dense(256,\n",
        "                      activation=\"relu\"),\n",
        "         # layers.Dense(128,\n",
        "         #              activation=\"relu\"),\n",
        "         # layers.Dense(64,\n",
        "         #              activation=\"relu\"),\n",
        "         layers.Dense(output_shape, \n",
        "                      activation = activation_function)\n",
        "        ]\n",
        "    )\n",
        "  elif getTypology()[1] is \"ram\":\n",
        "    return keras.Sequential(\n",
        "        [\n",
        "         layers.InputLayer(input_shape),\n",
        "         layers.LSTM(32,\n",
        "                     dropout=0.2),\n",
        "         layers.BatchNormalization(),\n",
        "         layers.Dropout(0.2),\n",
        "         layers.Dense(32,\n",
        "                      activation=\"relu\"\n",
        "                      ),\n",
        "         layers.Dropout(0.2),\n",
        "         layers.Dense(64,\n",
        "                      activation=\"relu\"),\n",
        "         layers.Dropout(0.2),\n",
        "         layers.Dense(32,\n",
        "                      activation=\"relu\"),\n",
        "         layers.Dense(output_shape,\n",
        "                      activation = activation_function)       \n",
        "        ]\n",
        "    )\n",
        "\n",
        "def create_model_classic(input_shape, output_shape, activation_function):\n",
        "  return keras.Sequential(\n",
        "      [\n",
        "       layers.Input(input_shape),\n",
        "       layers.Dropout(0.2),\n",
        "       layers.Dense(32, activation = \"relu\"),\n",
        "       layers.BatchNormalization(),\n",
        "       layers.Dropout(0.2),\n",
        "       layers.Dense(16, activation = \"relu\"),\n",
        "       layers.Dense(16, activation = \"relu\"),\n",
        "       layers.Dense(output_shape,\n",
        "                    activation = activation_function)\n",
        "      ]\n",
        "  )"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEiSKn2WORcL"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bghH0YluOXxI"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        env = gym.make(env_name())\n",
        "        self.state_shape = env.observation_space.shape\n",
        "        self.action_shape = env.action_space.n\n",
        "        env.close()\n",
        "        \n",
        "        self.global_actor = Actor(self.state_shape, self.action_shape)\n",
        "        self.global_critic = Critic(self.state_shape)\n",
        "\n",
        "    def train(self):\n",
        "        workers = []\n",
        "\n",
        "        for i in range(getNumberOfWorkers()):\n",
        "            workers.append(WorkerAgent(self.global_actor,\n",
        "                                       self.global_critic))\n",
        "        for worker in workers:\n",
        "            worker.start()\n",
        "\n",
        "        for worker in workers:\n",
        "            worker.join()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iacLy4soOTSE"
      },
      "source": [
        "# WorkerAgent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJONQe_4Oc7_"
      },
      "source": [
        "class WorkerAgent(Thread):\n",
        "    def __init__(self, global_actor, global_critic):\n",
        "        Thread.__init__(self)\n",
        "        self.lock = Lock()\n",
        "        self.env = gym.make(env_name())\n",
        "        # print(env.observation_space.shape)\n",
        "        # print(env.observation_space.shape[0])\n",
        "        self.state_shape = env.observation_space.shape\n",
        "        self.action_shape = env.action_space.n\n",
        "\n",
        "        self.global_actor = global_actor\n",
        "        self.global_critic = global_critic\n",
        "        self.actor = Actor(self.state_shape, self.action_shape)\n",
        "        self.critic = Critic(self.state_shape)\n",
        "\n",
        "        self.actor.model.set_weights(self.global_actor.model.get_weights())\n",
        "        self.critic.model.set_weights(self.global_critic.model.get_weights())\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        def list_to_batch(list):\n",
        "            batch = list[0]\n",
        "\n",
        "            for elem in list[1:]:\n",
        "                batch = np.append(batch, elem, axis = 0)\n",
        "            return batch\n",
        "\n",
        "        def n_step_td_target(rewards, next_v_value, done):\n",
        "            td_targets = np.zeros_like(rewards)\n",
        "            cumulative = 0.99 * next_v_value if not done else 0\n",
        "\n",
        "            for k in reversed(range(0, len(rewards))):\n",
        "            # for k in range(0, len(rewards)):\n",
        "                cumulative = pow(0.99, k) * cumulative + rewards[k]\n",
        "                td_targets[k] = cumulative\n",
        "            return td_targets\n",
        "\n",
        "        global CUR_EPISODE\n",
        "\n",
        "        while getMaxEpisodes() >= CUR_EPISODE:\n",
        "            state_batch = []\n",
        "            action_batch = []\n",
        "            reward_batch = []\n",
        "            episode_reward, episode_actor_loss, episode_critic_loss, done = 0, 0, 0, False\n",
        "\n",
        "            state = self.env.reset()\n",
        "\n",
        "            while not done:\n",
        "                # self.env.render()\n",
        "                action = self.actor.get_action(state)\n",
        "                next_state, reward, done, _ = self.env.step(action) \n",
        "                episode_reward += reward\n",
        "\n",
        "                if episode_reward >= 200:\n",
        "                  done = True\n",
        "                # print(reward)\n",
        "                # print(episode_reward)\n",
        "\n",
        "                # action = np.reshape(action, [1, 1])\n",
        "                # next_state = np.reshape(next_state, [1, self.state_shape])\n",
        "                # reward = reward + tf.math.reduce_sum(reward_batch) * pow(0.99, len(state_batch))\n",
        "                reward = np.reshape(reward, [1, 1])\n",
        "\n",
        "                state_batch.append(state)\n",
        "                action_batch.append(action)\n",
        "                reward_batch.append(reward)\n",
        "\n",
        "                if len(state_batch) >= getBatchSize() or done:\n",
        "                    actions = np.array(action_batch)\n",
        "                    # state_batch.append(next_state)\n",
        "                    states = np.array(state_batch)\n",
        "                    # actions = list_to_batch(action_batch)\n",
        "                    # states = list_to_batch(state_batch)\n",
        "                    rewards = list_to_batch(reward_batch)\n",
        "\n",
        "                    # td_targets = n_step_td_target(rewards, next_v_value, done)\n",
        "                    \n",
        "                    values = self.critic.get_value(states, single = False)\n",
        "                    final_value = self.critic.get_value(next_state, single = True)\n",
        "                    discounts = 0.99 * np.ones(values.shape)\n",
        "                    # print(values.shape)\n",
        "                    # print(final_value[0])\n",
        "                    # print(final_value[0].shape)\n",
        "                    # print(discounts.shape)\n",
        "                    # print(rewards.shape)\n",
        "                    # print(\"LOL\\n\\n\")\n",
        "                    advantages = value_ops.generalized_advantage_estimation(values,\n",
        "                                                                            final_value[0],\n",
        "                                                                            discounts, \n",
        "                                                                            rewards)\n",
        "                    # advantages = tf.keras.utils.normalize(advantages)\n",
        "                    # print(advantages.shape)\n",
        "                    # advantages = rewards + pow(0.99, len(state_batch)+1) * values[1:] - values[:-1])\n",
        "                    # states = states[:-1]\n",
        "                    \n",
        "                    # print(td_targets)\n",
        "                    # print(advantages)\n",
        "                    with self.lock:\n",
        "                        # print(states.shape)\n",
        "                        # print(actions.shape)\n",
        "                        # print(advantages.shape)\n",
        "                        actor_loss = self.global_actor.train(states, actions, advantages)\n",
        "                        self.actor.model.set_weights(self.global_actor.model.get_weights())\n",
        "\n",
        "                        critic_loss = self.global_critic.train(states, n_step_td_target(rewards, final_value, done))\n",
        "                        self.critic.model.set_weights(self.global_critic.model.get_weights())\n",
        "                        \n",
        "                        episode_actor_loss += actor_loss\n",
        "                        episode_critic_loss += critic_loss\n",
        "        \n",
        "                    state_batch = []\n",
        "                    action_batch = []\n",
        "                    reward_batch = []\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            if CUR_EPISODE%5 is 0:\n",
        "              print(\"EP{}: Reward = {}, Actor Loss = {}, Critic Loss = {}\".format(CUR_EPISODE, \n",
        "                                                                                  episode_reward, \n",
        "                                                                                  episode_actor_loss, \n",
        "                                                                                  episode_critic_loss))\n",
        "              self.actor.model.save(\"/content/drive/My Drive/Machine Learning/models/\" + str(env_name()) + \"_actor\")\n",
        "              self.critic.model.save(\"/content/drive/My Drive/Machine Learning/models/\" + str(env_name()) + \"_critic\")\n",
        "              \n",
        "              with open(\"/content/drive/My Drive/Machine Learning/logs/\" + str(env_name()) + \"_loss_actor\", \"w\") as csv_file:\n",
        "                csv_writer = csv.writer(csv_file, delimiter=',')\n",
        "                csv_writer.writerow([CUR_EPISODE, episode_actor_loss])\n",
        "\n",
        "              with open(\"/content/drive/My Drive/Machine Learning/logs/\" + str(env_name()) + \"_loss_critic\", \"w\") as csv_file:\n",
        "                csv_writer = csv.writer(csv_file, delimiter=',')\n",
        "                csv_writer.writerow([CUR_EPISODE, episode_critic_loss])\n",
        "\n",
        "              with open(\"/content/drive/My Drive/Machine Learning/logs/\" + str(env_name()) + \"_reward\", \"w\") as csv_file:\n",
        "                csv_writer = csv.writer(csv_file, delimiter=',')\n",
        "                csv_writer.writerow([CUR_EPISODE, episode_reward])\n",
        "              \n",
        "            CUR_EPISODE += 1\n",
        "\n",
        "    def run(self):\n",
        "        self.train()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_uiCGZBOea_"
      },
      "source": [
        "# Entrypoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLtYNO7PD_ZC",
        "outputId": "59cdfde5-94b4-4950-c91f-1a0c230acd89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "else:\n",
        "  main()\n",
        "\n",
        "# Load model"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EP0: Reward = 11.0, Actor Loss = 4.577521560192109, Critic Loss = 28.675021469593048\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "EP0: Reward = 30.0, Actor Loss = 12.140057108402253, Critic Loss = 86.16731643676758\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP5: Reward = 14.0, Actor Loss = 5.5110784590244295, Critic Loss = 31.14291000366211\n",
            "EP5: Reward = 32.0, Actor Loss = 12.297971955537797, Critic Loss = 81.5181280374527\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP10: Reward = 31.0, Actor Loss = 11.909870939850808, Critic Loss = 74.49057513475418\n",
            "EP10: Reward = 12.0, Actor Loss = 4.351761348247528, Critic Loss = 28.42538893222809\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP15: Reward = 28.0, Actor Loss = 10.462429429888724, Critic Loss = 62.25601863861084\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP15: Reward = 35.0, Actor Loss = 12.622001631259918, Critic Loss = 76.74470663070679\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP20: Reward = 16.0, Actor Loss = 5.495263068079948, Critic Loss = 25.671855449676514\n",
            "EP20: Reward = 11.0, Actor Loss = 4.314861450195313, Critic Loss = 20.517545491456985\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP25: Reward = 31.0, Actor Loss = 11.779769620895387, Critic Loss = 49.823710940778255\n",
            "EP25: Reward = 48.0, Actor Loss = 18.20683061361313, Critic Loss = 85.52691459655762\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP30: Reward = 21.0, Actor Loss = 7.523221522569656, Critic Loss = 32.72995361126959\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP30: Reward = 20.0, Actor Loss = 6.8921077722311015, Critic Loss = 28.047378540039062\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP35: Reward = 15.0, Actor Loss = 5.514582167863846, Critic Loss = 20.49890661239624\n",
            "EP35: Reward = 27.0, Actor Loss = 8.661334515213966, Critic Loss = 29.81112265586853\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP40: Reward = 14.0, Actor Loss = 5.206642064452172, Critic Loss = 15.414180278778076\n",
            "EP40: Reward = 23.0, Actor Loss = 7.9980988675355915, Critic Loss = 31.17190170288086\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP45: Reward = 10.0, Actor Loss = 3.6575910580158233, Critic Loss = 7.206353664398193\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP50: Reward = 17.0, Actor Loss = 5.125823231339455, Critic Loss = 24.01442813873291\n",
            "EP50: Reward = 20.0, Actor Loss = 6.142748591303826, Critic Loss = 20.69805145263672\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP55: Reward = 20.0, Actor Loss = 6.737201185226441, Critic Loss = 19.704896926879883\n",
            "EP55: Reward = 36.0, Actor Loss = 10.505374875068664, Critic Loss = 35.57571744918823\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP60: Reward = 19.0, Actor Loss = 6.84943510055542, Critic Loss = 13.242920398712158\n",
            "EP60: Reward = 24.0, Actor Loss = 8.234734796285629, Critic Loss = 24.159584999084473\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP65: Reward = 42.0, Actor Loss = 13.668392658233644, Critic Loss = 52.84978246688843\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP65: Reward = 41.0, Actor Loss = 12.054174283146859, Critic Loss = 47.92973208427429\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP70: Reward = 25.0, Actor Loss = 7.261181490421295, Critic Loss = 31.316033363342285\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP70: Reward = 54.0, Actor Loss = 16.61894024014473, Critic Loss = 51.43624258041382\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP75: Reward = 24.0, Actor Loss = 8.064149091243744, Critic Loss = 40.28182125091553\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP75: Reward = 36.0, Actor Loss = 11.215752702355385, Critic Loss = 27.303110599517822\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP80: Reward = 16.0, Actor Loss = 6.318754605054855, Critic Loss = 29.727270126342773\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP85: Reward = 73.0, Actor Loss = 22.766289621591564, Critic Loss = 115.16155242919922\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP85: Reward = 34.0, Actor Loss = 10.442074721455574, Critic Loss = 34.23261117935181\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP90: Reward = 38.0, Actor Loss = 9.130748608708382, Critic Loss = 53.89144945144653\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP95: Reward = 23.0, Actor Loss = 5.914802049994468, Critic Loss = 36.422722816467285\n",
            "EP95: Reward = 70.0, Actor Loss = 19.8523523825407, Critic Loss = 65.77994728088379\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP100: Reward = 19.0, Actor Loss = 4.675422250032425, Critic Loss = 15.961384296417236\n",
            "EP100: Reward = 26.0, Actor Loss = 7.32610325396061, Critic Loss = 35.95286417007446\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP105: Reward = 89.0, Actor Loss = 23.349053278565414, Critic Loss = 92.85226011276245\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP110: Reward = 44.0, Actor Loss = 12.227609043717385, Critic Loss = 84.1224775314331\n",
            "EP110: Reward = 41.0, Actor Loss = 10.611922271847726, Critic Loss = 67.62879371643066\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP115: Reward = 21.0, Actor Loss = 5.782078241705895, Critic Loss = 36.262619972229004\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP120: Reward = 23.0, Actor Loss = 6.853378800153732, Critic Loss = 43.91486740112305\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP125: Reward = 70.0, Actor Loss = 16.224874005913737, Critic Loss = 82.28340196609497\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP125: Reward = 94.0, Actor Loss = 24.693568277955055, Critic Loss = 147.21395206451416\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP130: Reward = 54.0, Actor Loss = 16.44272216618061, Critic Loss = 121.22218298912048\n",
            "EP130: Reward = 70.0, Actor Loss = 15.856744144558906, Critic Loss = 116.12929201126099\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP135: Reward = 73.0, Actor Loss = 18.4546055996418, Critic Loss = 217.4065179824829\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP140: Reward = 41.0, Actor Loss = 10.038757078647613, Critic Loss = 78.75916242599487\n",
            "EP140: Reward = 29.0, Actor Loss = 7.4706376051902765, Critic Loss = 54.85952663421631\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP145: Reward = 67.0, Actor Loss = 20.319085415005688, Critic Loss = 151.1176996231079\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP150: Reward = 63.0, Actor Loss = 14.50227454841137, Critic Loss = 196.61330890655518\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP155: Reward = 84.0, Actor Loss = 24.28587329298258, Critic Loss = 233.15944480895996\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP155: Reward = 29.0, Actor Loss = 6.331248167753219, Critic Loss = 83.20669651031494\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP160: Reward = 23.0, Actor Loss = 5.508340304493903, Critic Loss = 107.76550149917603\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP165: Reward = 109.0, Actor Loss = 18.904886323809624, Critic Loss = 251.95533800125122\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP165: Reward = 45.0, Actor Loss = 12.866034449636937, Critic Loss = 213.0997085571289\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP170: Reward = 58.0, Actor Loss = 18.79242927879095, Critic Loss = 219.79537725448608\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP175: Reward = 88.0, Actor Loss = 20.30292178183794, Critic Loss = 195.56495141983032\n",
            "EP175: Reward = 38.0, Actor Loss = 8.492517043352127, Critic Loss = 159.52162170410156\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP180: Reward = 118.0, Actor Loss = 18.484279663264747, Critic Loss = 202.64615488052368\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP185: Reward = 63.0, Actor Loss = 16.605047890245917, Critic Loss = 269.6282958984375\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP185: Reward = 54.0, Actor Loss = 10.682041689455508, Critic Loss = 160.5620551109314\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP190: Reward = 105.0, Actor Loss = 21.69557137876749, Critic Loss = 225.03017830848694\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP195: Reward = 42.0, Actor Loss = 11.037287808656691, Critic Loss = 190.563494682312\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP195: Reward = 62.0, Actor Loss = 11.969726003408432, Critic Loss = 175.98095273971558\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP200: Reward = 57.0, Actor Loss = 12.13298608481884, Critic Loss = 182.5396373271942\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP205: Reward = 63.0, Actor Loss = 15.556319759488106, Critic Loss = 318.77006340026855\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP210: Reward = 47.0, Actor Loss = 14.19768520832062, Critic Loss = 182.70809364318848\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP215: Reward = 58.0, Actor Loss = 15.153041468262671, Critic Loss = 291.0711040496826\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP220: Reward = 53.0, Actor Loss = 18.8634829133749, Critic Loss = 476.32151794433594\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP225: Reward = 51.0, Actor Loss = 11.99826322734356, Critic Loss = 207.26974391937256\n",
            "EP225: Reward = 34.0, Actor Loss = 9.526240772902966, Critic Loss = 369.76852798461914\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP230: Reward = 99.0, Actor Loss = 24.375715723335745, Critic Loss = 361.4734959602356\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP230: Reward = 66.0, Actor Loss = 19.29748913168907, Critic Loss = 518.8355903625488\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP235: Reward = 86.0, Actor Loss = 19.2023923227191, Critic Loss = 366.5706522464752\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP240: Reward = 86.0, Actor Loss = 19.51745995551348, Critic Loss = 292.84752368927\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP245: Reward = 84.0, Actor Loss = 22.332921641469003, Critic Loss = 539.7950639724731\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP250: Reward = 65.0, Actor Loss = 19.685957846343516, Critic Loss = 421.38654041290283\n",
            "EP250: Reward = 53.0, Actor Loss = 11.43923790335655, Critic Loss = 409.6361560821533\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP255: Reward = 50.0, Actor Loss = 11.083156801462174, Critic Loss = 183.19149684906006\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP260: Reward = 46.0, Actor Loss = 17.325056763887407, Critic Loss = 291.1417489051819\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP265: Reward = 52.0, Actor Loss = 16.22197375029325, Critic Loss = 418.9181213378906\n",
            "EP265: Reward = 40.0, Actor Loss = 11.2445880228281, Critic Loss = 225.03491401672363\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP270: Reward = 72.0, Actor Loss = 18.66789590269327, Critic Loss = 549.2942495346069\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP275: Reward = 119.0, Actor Loss = 26.32768847256899, Critic Loss = 321.4835720062256\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP280: Reward = 61.0, Actor Loss = 15.130473290681838, Critic Loss = 209.24544620513916\n",
            "EP280: Reward = 52.0, Actor Loss = 13.034908697903157, Critic Loss = 308.8903076648712\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP285: Reward = 47.0, Actor Loss = 11.668439556658267, Critic Loss = 231.86407470703125\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP290: Reward = 51.0, Actor Loss = 13.667704055309295, Critic Loss = 366.0703477859497\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP295: Reward = 55.0, Actor Loss = 13.225729878246783, Critic Loss = 258.76912212371826\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP295: Reward = 43.0, Actor Loss = 11.958798646926878, Critic Loss = 292.559693813324\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP300: Reward = 42.0, Actor Loss = 9.292762819230555, Critic Loss = 248.66703414916992\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP305: Reward = 41.0, Actor Loss = 11.398391525447368, Critic Loss = 326.844603061676\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP310: Reward = 96.0, Actor Loss = 26.447612949013713, Critic Loss = 260.30841541290283\n",
            "EP310: Reward = 42.0, Actor Loss = 11.444271946847438, Critic Loss = 290.86550426483154\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP315: Reward = 45.0, Actor Loss = 10.63412469893694, Critic Loss = 279.32064151763916\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP320: Reward = 67.0, Actor Loss = 16.50164300650358, Critic Loss = 211.62763595581055\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP320: Reward = 41.0, Actor Loss = 9.447635706365109, Critic Loss = 210.70613384246826\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP325: Reward = 72.0, Actor Loss = 18.1794540604949, Critic Loss = 160.47420692443848\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP330: Reward = 33.0, Actor Loss = 10.187403252124785, Critic Loss = 230.3692901134491\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP330: Reward = 73.0, Actor Loss = 17.23934926211834, Critic Loss = 471.57937932014465\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP335: Reward = 61.0, Actor Loss = 15.38432275623083, Critic Loss = 185.66151809692383\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP340: Reward = 75.0, Actor Loss = 18.258801139891148, Critic Loss = 249.03424263000488\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP345: Reward = 104.0, Actor Loss = 24.785364351868626, Critic Loss = 220.51909399032593\n",
            "EP345: Reward = 47.0, Actor Loss = 14.332273452579974, Critic Loss = 181.8362193107605\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP350: Reward = 86.0, Actor Loss = 21.212641285359858, Critic Loss = 402.1220736503601\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP350: Reward = 63.0, Actor Loss = 15.631497314870359, Critic Loss = 342.09654331207275\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP355: Reward = 40.0, Actor Loss = 13.879223516881465, Critic Loss = 169.46489906311035\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP355: Reward = 55.0, Actor Loss = 14.049058883190156, Critic Loss = 362.9488182067871\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP360: Reward = 53.0, Actor Loss = 15.362365489304066, Critic Loss = 235.30220079421997\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP365: Reward = 54.0, Actor Loss = 16.311633941829204, Critic Loss = 239.84841775894165\n",
            "EP365: Reward = 33.0, Actor Loss = 6.727829561829567, Critic Loss = 239.21829462051392\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP370: Reward = 50.0, Actor Loss = 9.763532982170581, Critic Loss = 194.58271026611328\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP375: Reward = 48.0, Actor Loss = 14.817597438395023, Critic Loss = 146.9543867111206\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP375: Reward = 28.0, Actor Loss = 6.338483707904816, Critic Loss = 120.74007797241211\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP380: Reward = 55.0, Actor Loss = 17.20452234953642, Critic Loss = 298.5416069030762\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP385: Reward = 53.0, Actor Loss = 16.941542426645753, Critic Loss = 234.32306480407715\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP390: Reward = 156.0, Actor Loss = 39.36985513418913, Critic Loss = 285.38195180892944\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP395: Reward = 37.0, Actor Loss = 10.111311627328396, Critic Loss = 142.0963544845581\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP400: Reward = 68.0, Actor Loss = 15.335921227633953, Critic Loss = 128.52601158618927\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP405: Reward = 70.0, Actor Loss = 14.503302055001257, Critic Loss = 136.70287609100342\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP405: Reward = 74.0, Actor Loss = 14.873530608713626, Critic Loss = 185.3230152130127\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP410: Reward = 77.0, Actor Loss = 17.39707705795765, Critic Loss = 295.3807301521301\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP415: Reward = 42.0, Actor Loss = 8.562909305095673, Critic Loss = 195.53601455688477\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP420: Reward = 38.0, Actor Loss = 7.1576999604702, Critic Loss = 115.63897562026978\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP420: Reward = 67.0, Actor Loss = 16.63324991196394, Critic Loss = 148.42000770568848\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP425: Reward = 27.0, Actor Loss = 4.454557394385338, Critic Loss = 143.65470504760742\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP430: Reward = 30.0, Actor Loss = 5.675387954711915, Critic Loss = 77.45748805999756\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP430: Reward = 53.0, Actor Loss = 9.044196763634682, Critic Loss = 164.42505502700806\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP435: Reward = 36.0, Actor Loss = 7.865486334264279, Critic Loss = 101.22889184951782\n",
            "EP435: Reward = 21.0, Actor Loss = 5.608657565116882, Critic Loss = 96.33993339538574\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP440: Reward = 43.0, Actor Loss = 7.617888880074024, Critic Loss = 104.91931486129761\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP440: Reward = 35.0, Actor Loss = 7.25965306699276, Critic Loss = 77.26464939117432\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP445: Reward = 56.0, Actor Loss = 13.850516720712184, Critic Loss = 225.71600151062012\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP445: Reward = 39.0, Actor Loss = 8.407277555763722, Critic Loss = 94.73332262039185\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP450: Reward = 56.0, Actor Loss = 10.063099109530448, Critic Loss = 113.864994764328\n",
            "EP450: Reward = 26.0, Actor Loss = 6.50172142982483, Critic Loss = 83.09739112854004\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP455: Reward = 30.0, Actor Loss = 5.898719190955162, Critic Loss = 71.34507942199707\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP460: Reward = 33.0, Actor Loss = 6.310033278167247, Critic Loss = 120.60041999816895\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP465: Reward = 48.0, Actor Loss = 9.699352084100246, Critic Loss = 112.17121505737305\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "EP465: Reward = 22.0, Actor Loss = 5.474485693573952, Critic Loss = 116.49248123168945\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP470: Reward = 40.0, Actor Loss = 9.519052369892597, Critic Loss = 83.21485757827759\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP475: Reward = 36.0, Actor Loss = 7.607758868932724, Critic Loss = 98.17591905593872\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP475: Reward = 26.0, Actor Loss = 5.109974103569984, Critic Loss = 86.29374027252197\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP480: Reward = 26.0, Actor Loss = 5.8854711738228795, Critic Loss = 61.530686378479004\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_critic/assets\n",
            "EP485: Reward = 38.0, Actor Loss = 8.553218889236451, Critic Loss = 136.36123275756836\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Machine Learning/models/CartPole-v1_actor/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b44013ccce4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-f0f88a129f6c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-4d82c9ab0de9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mworker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}