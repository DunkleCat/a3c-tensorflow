{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNEFue6fVDaJ3cZKiRFm0+i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DunkleCat/a3c-tensorflow/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj5ZKxV0N9Th",
        "colab_type": "text"
      },
      "source": [
        "# Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOyaCBsXOFh2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e696ee6-dccc-4810-9a2e-57569cf88c0f"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from threading import Thread, Lock\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X18YTkTZOEa2",
        "colab_type": "text"
      },
      "source": [
        "# ActorCritic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JiTpFEarvEO",
        "colab_type": "text"
      },
      "source": [
        "## Actor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxcQLC8kYZ-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor:\n",
        "\n",
        "    def __init__(self, state_shape, action_shape):\n",
        "      self.state_shape = np.ndarray(shape = (1,\n",
        "                                             state_shape[0],\n",
        "                                             state_shape[1],\n",
        "                                             state_shape[2])).shape\n",
        "      self.action_shape = action_shape\n",
        "      self.model = create_model(self.state_shape, self.action_shape)\n",
        "      self.opt = tf.keras.optimizers.Adam(0.0005)\n",
        "\n",
        "    def get_action(self, state):\n",
        "      tmp = np.ndarray(shape = (1,\n",
        "                                1,\n",
        "                                state.shape[0],\n",
        "                                state.shape[1],\n",
        "                                state.shape[2]))\n",
        "      tmp[0][0] = state\n",
        "      action_distribution = self.model.predict(tmp)\n",
        "      action = np.random.choice(self.action_shape, p = action_distribution[0])\n",
        "      return np.argmax(action_distribution == action)\n",
        "\n",
        "    def train(self, states, actions, advantages):\n",
        "      # print(\"Start Actor training with len {}\".format(len(states)))\n",
        "      # print(states.shape)\n",
        "      # print(states[0].shape)\n",
        "      tmp = np.ndarray(shape = (len(states),\n",
        "                              1,\n",
        "                              states[0].shape[0],\n",
        "                              states[0].shape[1],\n",
        "                              states[0].shape[2]))\n",
        "      for k in range(len(states)):\n",
        "        tmp[k][0] = states[k]\n",
        "\n",
        "      def compute_loss(actions, action_dist, advantages):\n",
        "        # Compute policy loss\n",
        "        scc = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
        "        policy_loss = scc(actions, action_dist, sample_weight = tf.stop_gradient(advantages))\n",
        "        # Compute entropy\n",
        "        cc = tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
        "        entropy = 0.01 * cc(action_dist, action_dist)\n",
        "        return policy_loss - entropy\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        action_dist = self.model(tmp, training = True)\n",
        "        loss = compute_loss(actions, action_dist, advantages)\n",
        "      grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "      self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "      return loss"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0rD3US-rtVC",
        "colab_type": "text"
      },
      "source": [
        "## Critic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pK8YGjpk_fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic:\n",
        "  def __init__(self, state_shape):\n",
        "    self.state_shape = np.ndarray(shape = (1,\n",
        "                                           state_shape[0],\n",
        "                                           state_shape[1],\n",
        "                                           state_shape[2])).shape\n",
        "    self.model = create_model(self.state_shape, 1)\n",
        "    self.opt = tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "  def get_values(self, states):\n",
        "    # print(states.shape)\n",
        "    tmp = np.ndarray(shape = (len(states),\n",
        "                              1,\n",
        "                              states[0].shape[0],\n",
        "                              states[0].shape[1],\n",
        "                              states[0].shape[2]))\n",
        "    for k in range(len(states)):\n",
        "      tmp[k][0] = states[k]\n",
        "    # print(tmp.shape)\n",
        "    return self.model.predict(tmp)\n",
        "\n",
        "  def get_value(self, state):\n",
        "    tmp = np.ndarray(shape = (1,\n",
        "                              1,\n",
        "                              state.shape[0],\n",
        "                              state.shape[1],\n",
        "                              state.shape[2]))\n",
        "    tmp[0][0] = state\n",
        "    return self.model.predict(tmp)\n",
        "\n",
        "\n",
        "  def train(self, states, td_targets):\n",
        "    # print(\"Start Critic training with len {}\".format(len(states)))\n",
        "    # print(states.shape)\n",
        "    # print(states[0].shape)\n",
        "    tmp = np.ndarray(shape = (len(states),\n",
        "                              1,\n",
        "                              states[0].shape[0],\n",
        "                              states[0].shape[1],\n",
        "                              states[0].shape[2]))\n",
        "    for k in range(len(states)):\n",
        "      tmp[k][0] = states[k]\n",
        "    # print(\"tmp.shape = {}\".format(tmp.shape))\n",
        "\n",
        "    def compute_loss(v_pred, v_targets):\n",
        "      mse = tf.keras.losses.MeanSquaredError()\n",
        "      return mse(td_targets, v_pred)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      v_pred = self.model(tmp, training=True)\n",
        "      # print(v_pred.shape)\n",
        "      # print(td_targets.shape)\n",
        "      assert v_pred.shape == td_targets.shape\n",
        "      loss = compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
        "    grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "    self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "    return loss"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT1ZYamwoibJ",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVmYqHAVlVjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(input_shape, output_shape):\n",
        "  # Input Layer\n",
        "  model = keras.Sequential([\n",
        "                            layers.InputLayer(input_shape),\n",
        "                            layers.ConvLSTM2D(64,4,2, \n",
        "                                              data_format='channels_last',\n",
        "                                              dropout = 0.5,\n",
        "                                              recurrent_dropout = 0.5,\n",
        "                                              return_sequences = True),\n",
        "                            layers.BatchNormalization(),\n",
        "                            # layers.ConvLSTM2D(48,4,1, \n",
        "                            #                   data_format='channels_last',\n",
        "                            #                   dropout = 0.5,\n",
        "                            #                   recurrent_dropout = 0.5,\n",
        "                            #                   return_sequences = True),\n",
        "                            # layers.BatchNormalization(),\n",
        "                            layers.ConvLSTM2D(32,4,2, \n",
        "                                              data_format='channels_last',\n",
        "                                              dropout = 0.5,\n",
        "                                              recurrent_dropout = 0.5,\n",
        "                                              return_sequences = True),\n",
        "                            layers.BatchNormalization(),\n",
        "                            layers.ConvLSTM2D(1,1,1, \n",
        "                                              data_format='channels_last',\n",
        "                                              dropout = 0.5,\n",
        "                                              recurrent_dropout = 0.5,\n",
        "                                              return_sequences = False),\n",
        "                            layers.BatchNormalization(),\n",
        "        \n",
        "                            layers.AveragePooling2D(),\n",
        "                            layers.Flatten(),\n",
        "                            layers.Dropout(0.5),\n",
        "                            layers.Dense(128)\n",
        "  ])\n",
        "        \n",
        "  if output_shape is 1:\n",
        "    model.add(layers.Dense(output_shape, activation = \"linear\"))\n",
        "  else:\n",
        "    model.add(layers.Dense(output_shape, activation = \"softmax\"))\n",
        "    \n",
        "  return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEiSKn2WORcL",
        "colab_type": "text"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bghH0YluOXxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, env_name):\n",
        "        env = gym.make(env_name)\n",
        "        self.env_name = env_name\n",
        "        self.state_shape = env.observation_space.shape\n",
        "        self.action_shape = env.action_space.n\n",
        "\n",
        "        self.global_actor = Actor(self.state_shape, self.action_shape)\n",
        "        self.global_critic = Critic(self.state_shape)\n",
        "        # self.num_workers = cpu_count()\n",
        "        self.num_workers = 1\n",
        "\n",
        "    def train(self, max_episodes = 1000):\n",
        "        workers = []\n",
        "\n",
        "        for i in range(self.num_workers):\n",
        "            env = gym.make(self.env_name)\n",
        "            workers.append(WorkerAgent(env, \n",
        "                                       self.global_actor,\n",
        "                                       self.global_critic, \n",
        "                                       max_episodes))\n",
        "\n",
        "        for worker in workers:\n",
        "            worker.start()\n",
        "\n",
        "        for worker in workers:\n",
        "            worker.join()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iacLy4soOTSE",
        "colab_type": "text"
      },
      "source": [
        "# WorkerAgent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJONQe_4Oc7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CUR_EPISODE = 0\n",
        "\n",
        "class WorkerAgent(Thread):\n",
        "    def __init__(self, env, global_actor, global_critic, max_episodes):\n",
        "        Thread.__init__(self)\n",
        "        self.lock = Lock()\n",
        "        self.env = env\n",
        "        self.state_shape = env.observation_space.shape\n",
        "        self.action_shape = env.action_space.n\n",
        "\n",
        "        self.max_episodes = max_episodes\n",
        "        self.global_actor = global_actor\n",
        "        self.global_critic = global_critic\n",
        "        self.actor = Actor(self.state_shape, self.action_shape)\n",
        "        self.critic = Critic(self.state_shape)\n",
        "\n",
        "        self.actor.model.set_weights(self.global_actor.model.get_weights())\n",
        "        self.critic.model.set_weights(self.global_critic.model.get_weights())\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        def list_to_batch(list):\n",
        "            batch = list[0]\n",
        "\n",
        "            for elem in list[1:]:\n",
        "                batch = np.append(batch, elem, axis = 0)\n",
        "            return batch\n",
        "\n",
        "        def n_step_td_target(rewards, next_v_value, done):\n",
        "            td_targets = np.zeros_like(rewards)\n",
        "            cumulative = 0 if not done else next_v_value\n",
        "\n",
        "            for k in reversed(range(0, len(rewards))):\n",
        "                cumulative = 0.99 * cumulative + rewards[k]\n",
        "                td_targets[k] = cumulative\n",
        "\n",
        "            return td_targets\n",
        "\n",
        "        global CUR_EPISODE\n",
        "\n",
        "        while self.max_episodes >= CUR_EPISODE:\n",
        "            state_batch = []\n",
        "            action_batch = []\n",
        "            reward_batch = []\n",
        "            episode_reward, episode_actor_loss, episode_critic_loss, done = 0, 0, 0, False\n",
        "\n",
        "            state = self.env.reset()\n",
        "\n",
        "            while not done:\n",
        "                # self.env.render()\n",
        "                action = self.actor.get_action(state)\n",
        "\n",
        "                next_state, reward, done, _ = self.env.step(action) \n",
        "\n",
        "                # state = np.reshape(state, [1, self.state_shape])\n",
        "                action = np.reshape(action, [1, 1])\n",
        "                # next_state = np.reshape(next_state, [1, self.state_shape])\n",
        "                reward = np.reshape(reward, [1, 1])\n",
        "\n",
        "                state_batch.append(state)\n",
        "                action_batch.append(action)\n",
        "                reward_batch.append(reward)\n",
        "\n",
        "                if len(state_batch) >= 5 or done:\n",
        "                    states = np.array(state_batch)\n",
        "                    # actions = list_to_batch(action_batch)\n",
        "                    rewards = list_to_batch(reward_batch)\n",
        "                    # states = state_batch\n",
        "                    actions = np.array(action_batch)\n",
        "                    # rewards = np.array(reward_batch)\n",
        "\n",
        "                    next_v_value = self.critic.get_value(next_state)\n",
        "                    td_targets = n_step_td_target(rewards, next_v_value, done)\n",
        "                    advantages = td_targets - self.critic.get_values(states)\n",
        "\n",
        "                    with self.lock:\n",
        "                        actor_loss = self.global_actor.train(states, actions, advantages)\n",
        "                        critic_loss = self.global_critic.train(states, td_targets)\n",
        "                        \n",
        "                        self.actor.model.set_weights(self.global_actor.model.get_weights())\n",
        "                        self.critic.model.set_weights(self.global_critic.model.get_weights())\n",
        "\n",
        "                        episode_actor_loss += actor_loss\n",
        "                        episode_critic_loss += critic_loss\n",
        "        \n",
        "                    state_batch = []\n",
        "                    action_batch = []\n",
        "                    reward_batch = []\n",
        "\n",
        "                episode_reward += reward\n",
        "                state = next_state\n",
        "\n",
        "            print(\"EP{}: Reward = {}, Actor Loss = {}, Critic Loss = {}\".format(CUR_EPISODE, episode_reward, episode_actor_loss, episode_critic_loss))\n",
        "            # wandb.log({'Reward': episode_reward})\n",
        "            CUR_EPISODE += 1\n",
        "\n",
        "    def run(self):\n",
        "        self.train()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_uiCGZBOea_",
        "colab_type": "text"
      },
      "source": [
        "# Entrypoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfJxKa2Ql3rC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "8ec9a83e-94c8-4828-b91f-43004e8ea4e8"
      },
      "source": [
        "def getNumberOfWorkers():\n",
        "    return cpu_count()\n",
        "\n",
        "def getMaxEpisodes():\n",
        "    return 1000\n",
        "\n",
        "def environment():\n",
        "    return \"Pong-v0\"\n",
        "\n",
        "def main():\n",
        "    env_name = \"Pong-v0\"\n",
        "    agent = Agent(env_name)\n",
        "    agent.train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EP0: Reward = [[-21.]], Actor Loss = 25.70464836622123, Critic Loss = 190.54473880678415\n",
            "EP1: Reward = [[-21.]], Actor Loss = 2.078670869753698, Critic Loss = 44.56169947050512\n",
            "EP2: Reward = [[-21.]], Actor Loss = -27.527088342070343, Critic Loss = 34.52162306010723\n",
            "EP3: Reward = [[-21.]], Actor Loss = -15.171518729516832, Critic Loss = 26.13019384117797\n",
            "EP4: Reward = [[-21.]], Actor Loss = -26.70156973910518, Critic Loss = 24.463142310269177\n",
            "EP5: Reward = [[-21.]], Actor Loss = -21.974221642185007, Critic Loss = 22.49769447534345\n",
            "EP6: Reward = [[-21.]], Actor Loss = 24.044484234116972, Critic Loss = 18.790567808551714\n",
            "EP7: Reward = [[-21.]], Actor Loss = -28.70336722143388, Critic Loss = 17.655923280864954\n",
            "EP8: Reward = [[-21.]], Actor Loss = 54.03293051749467, Critic Loss = 16.712781139533035\n",
            "EP9: Reward = [[-21.]], Actor Loss = 30.869076483903935, Critic Loss = 15.372003926895559\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}